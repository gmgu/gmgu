### Research Interests
**Deep Learning**: AI Coding Assistant. Code LLM. Multi-Node Distributed Training. Parameter Efficient Fine-Tuning. Instruction Tuning. LLM Inference Server. Prompt Engineering. Benchmark Dataset. Data Collection and Cleaning. Time Series Forecasting. Semantic Parsing.

**Algorithm Engineering**: Fast and Scalable Algorithms. Graph Isomorphism. Subgraph Matching. Multiple String Matching. Order-Preserving Matching. Cartesian Tree Matching.

### Work Experience
**LG Electronics - Artificial Intelligence Lab (Senior Researcher)**
- Jan. 2024 - Present: **Development of AI Coding Assistant using Large Language Model**
  - onducting research on continual pretraining code LLMs in data scarce scenario.
  - Maintaining custom benchmark dataset for offline evaluation.
  - Analyzing user statistics and feedback for online evaluation.
  - Constructing instruction dataset and conducting instruction-tuning.
  - Prompt engineering for accurate code suggestion.
- Aug. 2022 – Dec. 2023: **Development of AI Coding Assistant using Large Language Model**
  - Conducted distributed training of LLMs based on decoder-only transformer on AWS.
  - Filtered and deduplicated terabytes of source code data.
  - Conducted research about data augmentation, forgetting, and efficient fine-tuning of LLM.
  - Developed a fast LLM inference server based on NVIDIA Triton and FasterTransformer.
- Apr. 2022 – Dec. 2022: **Development of Coding Education Program Utilizing AI**
  - Constructed training data for generating Python code from natural language instruction.
  - Trained an encoder-decoder transformer from scratch.
  - Developed a web client that inputs prompt, prints AI-generated code, and executes Python code.
  - Created a inference server that runs on multiple GPUs, loads multiple copies of the model, and offers dynamic batching for increased throughput.

**Seoul National University – Institute of Computer Technology (Post-Doctoral Assistant)**
- Sept. 2021 and Jan. 2022 – Mar. 2022: **Algorithm Development for Graph Isomorphism Query Processing**
  - Developed a fast graph isomorphism query processing algorithm that runs orders of magnitude faster than state-of-the-art algorithms.

**NAVER – AI Dev2 (Internship)**
- Oct. 2021: **Analyzing Conversion Tracking Data**
  - Conducted exploratory data analysis on glad for advertisement data to find meaningful trends.
  - Handled hundred gigabytes of (raw) conversion tracking data.
  - Solved optimization problem of maximizing conversion rate using linear programming.

### Skills
**Programming Languages**
- [C/C++](https://github.com/gmgu/GI), Python, [CUDA C++](https://github.com/gmgu/study-cuda), [Rust](https://github.com/gmgu/study-rust), C#, Java, Shell Script, LaTeX

**Libraries**
- PyTorch, TensorFlow, HuggingFace Transformers, DeepSpeed, Triton (NVIDIA), FasterTransformer, FastAPI, [Triton (OpenAI)](https://github.com/gmgu/study-triton), Seaborn, Pandas, PySpark, gtest

**Competitive Programming**

[![Solved.ac
프로필](http://mazassumnida.wtf/api/v2/generate_badge?boj=gmgu)](https://solved.ac/gmgu)

**Others**
- AWS (SageMaker, EC2, Lustre, S3)

### CV
[GeonmoGu_CV](https://github.com/gmgu/gmgu/blob/main/GeonmoGu_CV.pdf)
 
